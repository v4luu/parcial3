{"filter":false,"title":"procesador.py","tooltip":"/lambda2/procesador.py","undoManager":{"mark":12,"position":12,"stack":[[{"start":{"row":0,"column":0},"end":{"row":132,"column":0},"action":"insert","lines":["import boto3","import os","import csv","import io","import json","import urllib.parse","from bs4 import BeautifulSoup","from datetime import datetime","","# Clientes AWS","s3   = boto3.client('s3')","glue = boto3.client('glue')","","# Variables de entorno","BUCKET       = os.environ['BUCKET_NAME']","CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","","def app(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","","    # Para cada archivo .html subido","    for record in event.get('Records', []):","        bucket = record['s3']['bucket']['name']","        key    = urllib.parse.unquote_plus(record['s3']['object']['key'])","","        # Filtrar solo los raw HTML","        if not key.startswith('headlines/raw/') or not key.endswith('.html'):","            print(f\"Ignorando archivo no válido: {key}\")","            continue","","        print(f\"Procesando HTML: s3://{bucket}/{key}\")","        try:","            resp    = s3.get_object(Bucket=bucket, Key=key)","            content = resp['Body'].read().decode('utf-8')","        except Exception as e:","            print(f\"Error al leer {key}: {e}\")","            continue","","        soup = BeautifulSoup(content, 'html.parser')","        nombre_archivo = os.path.basename(key)","","        # Detectar periódico y URL base","        if 'publimetro' in nombre_archivo:","            periodico, base_url = 'publimetro', 'https://www.publimetro.co'","        elif 'eltiempo' in nombre_archivo:","            periodico, base_url = 'eltiempo', 'https://www.eltiempo.com'","        else:","            periodico, base_url = 'desconocido', ''","","        noticias = []","        # Extraer dentro de <article>","        for article in soup.find_all('article'):","            t = article.find(['h2', 'h3'])","            a = article.find('a', href=True)","            if not t or not a:","                continue","            titular = t.get_text(strip=True)","            enlace  = a['href']","            if not enlace.startswith('http') and base_url:","                enlace = base_url + enlace","            parts    = enlace.split('/')","            categoria = parts[3] if len(parts) > 3 else ''","            noticias.append({","                'categoria': categoria,","                'titular':   titular,","                'enlace':    enlace","            })","","        # Fallback si no se encontró nada en <article>","        if not noticias:","            print(\"Fallback a headings\")","            for heading in soup.find_all(['h2', 'h3']):","                a = heading.find('a', href=True)","                if not a:","                    continue","                titular = heading.get_text(strip=True)","                enlace  = a['href']","                if not enlace.startswith('http') and base_url:","                    enlace = base_url + enlace","                parts    = enlace.split('/')","                categoria = parts[3] if len(parts) > 3 else ''","                noticias.append({","                    'categoria': categoria,","                    'titular':   titular,","                    'enlace':    enlace","                })","","        print(f\"Total noticias extraídas: {len(noticias)} del periódico {periodico}\")","","        # Construir la ruta del CSV","        now   = datetime.utcnow()","        year  = now.strftime('%Y')","        month = now.strftime('%m')","        day   = now.strftime('%d')","        hour  = now.strftime('%H')","        minute= now.strftime('%M')","","        csv_key = (","            f\"headlines/final/periodico={periodico}/\"","            f\"year={year}/month={month}/day={day}/\"","            f\"noticias_{hour}-{minute}.csv\"","        )","        print(f\"Guardando CSV en: {csv_key}\")","","        # Escribir CSV en memoria y subirlo","        buffer_csv = io.StringIO()","        writer     = csv.DictWriter(buffer_csv, fieldnames=['categoria', 'titular', 'enlace'])","        writer.writeheader()","        for noticia in noticias:","            writer.writerow(noticia)","","        try:","            s3.put_object(Bucket=bucket, Key=csv_key, Body=buffer_csv.getvalue())","            print(\"CSV subido exitosamente.\")","        except Exception as e:","            print(f\"Error subiendo CSV: {e}\")","            continue","","        # Arrancar Glue Crawler inmediatamente","        print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'…\")","        try:","            resp   = glue.start_crawler(Name=CRAWLER_NAME)","            status = resp['ResponseMetadata']['HTTPStatusCode']","            print(f\"Crawler iniciado, HTTPStatusCode={status}\")","        except Exception as e:","            print(f\"Error arrancando Glue Crawler: {e}\")","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({\"message\": \"Procesamiento y crawler ejecutados\"})","    }",""],"id":1}],[{"start":{"row":0,"column":0},"end":{"row":132,"column":0},"action":"remove","lines":["import boto3","import os","import csv","import io","import json","import urllib.parse","from bs4 import BeautifulSoup","from datetime import datetime","","# Clientes AWS","s3   = boto3.client('s3')","glue = boto3.client('glue')","","# Variables de entorno","BUCKET       = os.environ['BUCKET_NAME']","CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","","def app(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","","    # Para cada archivo .html subido","    for record in event.get('Records', []):","        bucket = record['s3']['bucket']['name']","        key    = urllib.parse.unquote_plus(record['s3']['object']['key'])","","        # Filtrar solo los raw HTML","        if not key.startswith('headlines/raw/') or not key.endswith('.html'):","            print(f\"Ignorando archivo no válido: {key}\")","            continue","","        print(f\"Procesando HTML: s3://{bucket}/{key}\")","        try:","            resp    = s3.get_object(Bucket=bucket, Key=key)","            content = resp['Body'].read().decode('utf-8')","        except Exception as e:","            print(f\"Error al leer {key}: {e}\")","            continue","","        soup = BeautifulSoup(content, 'html.parser')","        nombre_archivo = os.path.basename(key)","","        # Detectar periódico y URL base","        if 'publimetro' in nombre_archivo:","            periodico, base_url = 'publimetro', 'https://www.publimetro.co'","        elif 'eltiempo' in nombre_archivo:","            periodico, base_url = 'eltiempo', 'https://www.eltiempo.com'","        else:","            periodico, base_url = 'desconocido', ''","","        noticias = []","        # Extraer dentro de <article>","        for article in soup.find_all('article'):","            t = article.find(['h2', 'h3'])","            a = article.find('a', href=True)","            if not t or not a:","                continue","            titular = t.get_text(strip=True)","            enlace  = a['href']","            if not enlace.startswith('http') and base_url:","                enlace = base_url + enlace","            parts    = enlace.split('/')","            categoria = parts[3] if len(parts) > 3 else ''","            noticias.append({","                'categoria': categoria,","                'titular':   titular,","                'enlace':    enlace","            })","","        # Fallback si no se encontró nada en <article>","        if not noticias:","            print(\"Fallback a headings\")","            for heading in soup.find_all(['h2', 'h3']):","                a = heading.find('a', href=True)","                if not a:","                    continue","                titular = heading.get_text(strip=True)","                enlace  = a['href']","                if not enlace.startswith('http') and base_url:","                    enlace = base_url + enlace","                parts    = enlace.split('/')","                categoria = parts[3] if len(parts) > 3 else ''","                noticias.append({","                    'categoria': categoria,","                    'titular':   titular,","                    'enlace':    enlace","                })","","        print(f\"Total noticias extraídas: {len(noticias)} del periódico {periodico}\")","","        # Construir la ruta del CSV","        now   = datetime.utcnow()","        year  = now.strftime('%Y')","        month = now.strftime('%m')","        day   = now.strftime('%d')","        hour  = now.strftime('%H')","        minute= now.strftime('%M')","","        csv_key = (","            f\"headlines/final/periodico={periodico}/\"","            f\"year={year}/month={month}/day={day}/\"","            f\"noticias_{hour}-{minute}.csv\"","        )","        print(f\"Guardando CSV en: {csv_key}\")","","        # Escribir CSV en memoria y subirlo","        buffer_csv = io.StringIO()","        writer     = csv.DictWriter(buffer_csv, fieldnames=['categoria', 'titular', 'enlace'])","        writer.writeheader()","        for noticia in noticias:","            writer.writerow(noticia)","","        try:","            s3.put_object(Bucket=bucket, Key=csv_key, Body=buffer_csv.getvalue())","            print(\"CSV subido exitosamente.\")","        except Exception as e:","            print(f\"Error subiendo CSV: {e}\")","            continue","","        # Arrancar Glue Crawler inmediatamente","        print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'…\")","        try:","            resp   = glue.start_crawler(Name=CRAWLER_NAME)","            status = resp['ResponseMetadata']['HTTPStatusCode']","            print(f\"Crawler iniciado, HTTPStatusCode={status}\")","        except Exception as e:","            print(f\"Error arrancando Glue Crawler: {e}\")","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({\"message\": \"Procesamiento y crawler ejecutados\"})","    }",""],"id":2},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":3},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":4},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":5},{"start":{"row":0,"column":0},"end":{"row":161,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","glue = boto3.client('glue')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","            # Arrancar Glue Crawler inmediatamente","            print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'...\")","            try:","                resp = glue.start_crawler(Name=CRAWLER_NAME)","                status = resp['ResponseMetadata']['HTTPStatusCode']","                print(f\"Crawler iniciado, HTTPStatusCode={status}\")","            except Exception as e:","                print(f\"Error arrancando Glue Crawler: {e}\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":161,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","glue = boto3.client('glue')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","            # Arrancar Glue Crawler inmediatamente","            print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'...\")","            try:","                resp = glue.start_crawler(Name=CRAWLER_NAME)","                status = resp['ResponseMetadata']['HTTPStatusCode']","                print(f\"Crawler iniciado, HTTPStatusCode={status}\")","            except Exception as e:","                print(f\"Error arrancando Glue Crawler: {e}\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":6},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":7},{"start":{"row":0,"column":0},"end":{"row":161,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","glue = boto3.client('glue')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","            # Arrancar Glue Crawler inmediatamente","            print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'...\")","            try:","                resp = glue.start_crawler(Name=CRAWLER_NAME)","                status = resp['ResponseMetadata']['HTTPStatusCode']","                print(f\"Crawler iniciado, HTTPStatusCode={status}\")","            except Exception as e:","                print(f\"Error arrancando Glue Crawler: {e}\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":161,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","glue = boto3.client('glue')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    CRAWLER_NAME = os.environ.get('CRAWLER_NAME', 'noticias')","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","            # Arrancar Glue Crawler inmediatamente","            print(f\"Arrancando Glue Crawler '{CRAWLER_NAME}'...\")","            try:","                resp = glue.start_crawler(Name=CRAWLER_NAME)","                status = resp['ResponseMetadata']['HTTPStatusCode']","                print(f\"Crawler iniciado, HTTPStatusCode={status}\")","            except Exception as e:","                print(f\"Error arrancando Glue Crawler: {e}\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":8},{"start":{"row":0,"column":0},"end":{"row":22,"column":16},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Configuraciones con valores por defecto robustos","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    ","    # Validación"]}],[{"start":{"row":0,"column":0},"end":{"row":22,"column":16},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Configuraciones con valores por defecto robustos","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    ","    # Validación"],"id":9},{"start":{"row":0,"column":0},"end":{"row":136,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Configuraciones con valores por defecto robustos","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    ","    # Validación inicial del evento","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'\"","        print(error_msg)","        return {\"statusCode\": 400, \"body\": json.dumps({\"error\": error_msg})}","","    resultados = []","    ","    for record in event['Records']:","        try:","            # Extracción de metadatos","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","            ","            # Validación de archivo HTML","            if not (key.startswith('headlines/raw/') and key.endswith('.html')):","                print(f\"Archivo no válido, ignorando: {key}\")","                continue","","            print(f\"Procesando: s3://{bucket}/{key}\")","            ","            # Descarga y parseo del HTML","            try:","                response = s3.get_object(Bucket=bucket, Key=key)","                content = response['Body'].read().decode('utf-8')","            except Exception as e:","                print(f\"Error leyendo HTML: {str(e)}\")","                continue","","            soup = BeautifulSoup(content, 'html.parser')","            filename = os.path.basename(key)","            ","            # Detección del periódico","            periodico = next(","                (p for p in BASE_URLS.keys() if p in filename),","                'desconocido'","            )","            ","            # Extracción de noticias","            noticias = []","            ","            # Método principal (article tags)","            for article in soup.find_all('article'):","                if (titular := article.find(['h1', 'h2', 'h3'])) and (enlace := article.find('a', href=True)):","                    url = enlace['href']","                    if not url.startswith('http'):","                        url = f\"{BASE_URLS.get(periodico, '')}{url}\"","                    ","                    noticias.append({","                        'categoria': url.split('/')[3] if len(url.split('/')) > 3 else '',","                        'titular': titular.get_text(strip=True),","                        'enlace': url","                    })","","            # Fallback (headings)","            if not noticias:","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    if (a := heading.find('a', href=True)):","                        url = a['href']","                        if not url.startswith('http'):","                            url = f\"{BASE_URLS.get(periodico, '')}{url}\"","                        ","                        noticias.append({","                            'categoria': url.split('/')[3] if len(url.split('/')) > 3 else '',","                            'titular': heading.get_text(strip=True),","                            'enlace': url","                        })","","            print(f\"Extraídas {len(noticias)} noticias de {periodico}\")","            ","            # Generación de ruta CSV","            now = datetime.utcnow()","            csv_key = (","                f\"headlines/final/periodico={periodico}/\"","                f\"year={now.year}/month={now.month:02d}/\"","                f\"day={now.day:02d}/noticias_{now.hour:02d}-{now.minute:02d}.csv\"","            )","            ","            # Creación y subida del CSV","            with io.StringIO() as csv_buffer:","                writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","                writer.writeheader()","                writer.writerows(noticias)","                ","                try:","                    s3.put_object(","                        Bucket=BUCKET_NAME,","                        Key=csv_key,","                        Body=csv_buffer.getvalue(),","                        ContentType='text/csv',","                        ACL='bucket-owner-full-control'  # Asegura permisos","                    )","                    print(f\"CSV guardado en s3://{BUCKET_NAME}/{csv_key}\")","                    resultados.append({","                        'periodico': periodico,","                        'archivo_origen': key,","                        'archivo_destino': csv_key,","                        'noticias': len(noticias)","                    })","                except Exception as e:","                    print(f\"Error subiendo CSV: {str(e)}\")","                    continue","","        except Exception as e:","            print(f\"Error procesando record: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Proceso completado\",","            \"resultados\": resultados,","            \"total_archivos\": len(resultados)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":136,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Configuraciones con valores por defecto robustos","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    ","    # Validación inicial del evento","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'\"","        print(error_msg)","        return {\"statusCode\": 400, \"body\": json.dumps({\"error\": error_msg})}","","    resultados = []","    ","    for record in event['Records']:","        try:","            # Extracción de metadatos","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","            ","            # Validación de archivo HTML","            if not (key.startswith('headlines/raw/') and key.endswith('.html')):","                print(f\"Archivo no válido, ignorando: {key}\")","                continue","","            print(f\"Procesando: s3://{bucket}/{key}\")","            ","            # Descarga y parseo del HTML","            try:","                response = s3.get_object(Bucket=bucket, Key=key)","                content = response['Body'].read().decode('utf-8')","            except Exception as e:","                print(f\"Error leyendo HTML: {str(e)}\")","                continue","","            soup = BeautifulSoup(content, 'html.parser')","            filename = os.path.basename(key)","            ","            # Detección del periódico","            periodico = next(","                (p for p in BASE_URLS.keys() if p in filename),","                'desconocido'","            )","            ","            # Extracción de noticias","            noticias = []","            ","            # Método principal (article tags)","            for article in soup.find_all('article'):","                if (titular := article.find(['h1', 'h2', 'h3'])) and (enlace := article.find('a', href=True)):","                    url = enlace['href']","                    if not url.startswith('http'):","                        url = f\"{BASE_URLS.get(periodico, '')}{url}\"","                    ","                    noticias.append({","                        'categoria': url.split('/')[3] if len(url.split('/')) > 3 else '',","                        'titular': titular.get_text(strip=True),","                        'enlace': url","                    })","","            # Fallback (headings)","            if not noticias:","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    if (a := heading.find('a', href=True)):","                        url = a['href']","                        if not url.startswith('http'):","                            url = f\"{BASE_URLS.get(periodico, '')}{url}\"","                        ","                        noticias.append({","                            'categoria': url.split('/')[3] if len(url.split('/')) > 3 else '',","                            'titular': heading.get_text(strip=True),","                            'enlace': url","                        })","","            print(f\"Extraídas {len(noticias)} noticias de {periodico}\")","            ","            # Generación de ruta CSV","            now = datetime.utcnow()","            csv_key = (","                f\"headlines/final/periodico={periodico}/\"","                f\"year={now.year}/month={now.month:02d}/\"","                f\"day={now.day:02d}/noticias_{now.hour:02d}-{now.minute:02d}.csv\"","            )","            ","            # Creación y subida del CSV","            with io.StringIO() as csv_buffer:","                writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","                writer.writeheader()","                writer.writerows(noticias)","                ","                try:","                    s3.put_object(","                        Bucket=BUCKET_NAME,","                        Key=csv_key,","                        Body=csv_buffer.getvalue(),","                        ContentType='text/csv',","                        ACL='bucket-owner-full-control'  # Asegura permisos","                    )","                    print(f\"CSV guardado en s3://{BUCKET_NAME}/{csv_key}\")","                    resultados.append({","                        'periodico': periodico,","                        'archivo_origen': key,","                        'archivo_destino': csv_key,","                        'noticias': len(noticias)","                    })","                except Exception as e:","                    print(f\"Error subiendo CSV: {str(e)}\")","                    continue","","        except Exception as e:","            print(f\"Error procesando record: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Proceso completado\",","            \"resultados\": resultados,","            \"total_archivos\": len(resultados)","        })","    }"],"id":10},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":11},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":12},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}],[{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"],"id":13},{"start":{"row":0,"column":0},"end":{"row":150,"column":5},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","import urllib.parse","import json","import io","","# Inicializar cliente S3","s3 = boto3.client('s3')","","def lambda_handler(event, context):","    print(\"==== Evento recibido por Lambda ====\")","    print(json.dumps(event, indent=2))","    ","    # Obtener configuraciones de variables de entorno","    BUCKET_NAME = os.environ.get('BUCKET_NAME', 'parci4l3')  # Valor por defecto 'parci4l3'","    BASE_URLS = {","        'publimetro': os.environ.get('PUBLIMETRO_URL', 'https://www.publimetro.co'),","        'eltiempo': os.environ.get('ELTIEMPO_URL', 'https://www.eltiempo.com')","    }","    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()","","    # Verificar si el evento tiene la estructura esperada","    if 'Records' not in event:","        error_msg = \"Evento no contiene 'Records'. Formato inesperado.\"","        print(error_msg)","        return {","            \"statusCode\": 400,","            \"body\": json.dumps({\"error\": error_msg})","        }","","    for record in event['Records']:","        try:","            bucket = record['s3']['bucket']['name']","            key = urllib.parse.unquote_plus(record['s3']['object']['key'])","","            if not key.startswith('headlines/raw/') or not key.endswith('.html'):","                print(f\"Ignorando archivo no válido para procesamiento: {key}\")","                continue","","            print(f\"Procesando archivo S3: bucket={bucket}, key={key}\")","","            # Obtener el contenido del archivo HTML","            response = s3.get_object(Bucket=bucket, Key=key)","            content = response['Body'].read().decode('utf-8')","            print(f\"Archivo HTML extraído correctamente: {key}\")","","            # Parsear el HTML","            soup = BeautifulSoup(content, 'html.parser')","            nombre_archivo = os.path.basename(key)","","            # Detectar el periódico por el nombre del archivo","            if 'publimetro' in nombre_archivo:","                periodico = 'publimetro'","            elif 'eltiempo' in nombre_archivo:","                periodico = 'eltiempo'","            else:","                periodico = 'desconocido'","                print(f\"Periódico no reconocido en el archivo: {nombre_archivo}\")","","            noticias = []","","            # Extraer noticias de etiquetas <article>","            for article in soup.find_all('article'):","                titular_tag = article.find(['h1', 'h2', 'h3'])","                enlace_tag = article.find('a', href=True)","","                if titular_tag and enlace_tag:","                    titular = titular_tag.get_text(strip=True)","                    enlace = enlace_tag['href']","                    ","                    if not enlace.startswith('http'):","                        enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                    categoria = ''","                    parts = enlace.split('/')","                    if len(parts) > 3:","                        categoria = parts[3]","","                    noticias.append({","                        'categoria': categoria,","                        'titular': titular,","                        'enlace': enlace","                    })","","            # Fallback si no se encontraron noticias en <article>","            if not noticias:","                print(\"No se encontraron artículos válidos en <article>. Se intenta fallback...\")","                for heading in soup.find_all(['h1', 'h2', 'h3']):","                    a_tag = heading.find('a', href=True)","                    if a_tag:","                        titular = heading.get_text(strip=True)","                        enlace = a_tag['href']","                        ","                        if not enlace.startswith('http'):","                            enlace = f\"{BASE_URLS.get(periodico, '')}{enlace}\"","","                        categoria = ''","                        parts = enlace.split('/')","                        if len(parts) > 3:","                            categoria = parts[3]","","                        noticias.append({","                            'categoria': categoria,","                            'titular': titular,","                            'enlace': enlace","                        })","","            print(f\"Total de noticias extraídas: {len(noticias)} del periódico {periodico}\")","","            # Generar estructura de carpetas por fecha","            fecha = datetime.utcnow()","            year = fecha.strftime('%Y')","            month = fecha.strftime('%m')","            day = fecha.strftime('%d')","            hour = fecha.strftime('%H')","            minute = fecha.strftime('%M')","","            # Ruta para el archivo CSV","            csv_key = f\"headlines/final/periodico={periodico}/year={year}/month={month}/day={day}/noticias_{hour}-{minute}.csv\"","            print(f\"Guardando CSV en: {csv_key}\")","","            # Crear CSV en memoria","            csv_buffer = io.StringIO()","            writer = csv.DictWriter(csv_buffer, fieldnames=['categoria', 'titular', 'enlace'])","            writer.writeheader()","            for noticia in noticias:","                writer.writerow(noticia)","","            # Subir CSV a S3","            s3.put_object(","                Bucket=bucket,","                Key=csv_key,","                Body=csv_buffer.getvalue(),","                ContentType='text/csv'","            )","            print(\"Archivo CSV subido exitosamente a S3.\")","","        except Exception as e:","            print(f\"Error procesando el archivo {key}: {str(e)}\")","            continue","","    return {","        \"statusCode\": 200,","        \"body\": json.dumps({","            \"message\": \"Procesamiento completado\",","            \"noticias_procesadas\": sum(1 for record in event['Records'] if 's3' in record)","        })","    }"]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":150,"column":5},"end":{"row":150,"column":5},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748576290192,"hash":"a3f26a6a45a821c008e8e034b09b5288e4a9049d"}